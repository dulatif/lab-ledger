# Lesson 3: Classification

## Learning Goals

- Logistic Regression.
- Decision Trees.

## 1. Logistic Regression

Predicting a category (spam/ham, yes/no).
Uses the **Sigmoid** function to squash output between 0 and 1 (probability).
Decision Boundary: If output > 0.5, class 1. Else, class 0.

## 2. K-Nearest Neighbors (KNN)

"Tell me who your friends are, and I'll tell you who you are."
Finds the K closest data points and votes.
Slow prediction (Lazy Learning).

## 3. Decision Trees

A flowchart of if-else rules.
`If Age > 30 and Income > 50k -> Buy`.
Highly interpretable. Prone to overfitting.

## Key Takeaways

- Logistic Regression is linear. Trees are non-linear.
- Choose simple models first (Occam's Razor).
