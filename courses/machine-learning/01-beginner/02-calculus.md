# Lesson 2: Calculus

## Learning Goals

- Derivatives.
- Gradient Descent.

## 1. The Derivative

What is the derivative? It is the **Rate of Change**.
If $y = x^2$, then $dy/dx = 2x$.
This tells us: "If we nudge x slightly, how much does y change?".

## 2. Partial Derivatives

For functions with multiple inputs ($f(x, y)$), the partial derivative measures how the output changes when we move _just one_ input while holding others constant.

## 3. The Gradient

The vector of all partial derivatives.
It points in the direction of steepest ascent.
To minimize error (Loss), we walk in the _opposite_ direction of the gradient.

## Key Takeaways

- ML is "Optimization".
- We use Calculus to find the parameters (weights) that minimize the error.
