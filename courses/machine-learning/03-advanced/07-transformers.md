# Lesson 7: Attention & Transformers

## Learning Goals

- Attention Mechanism.
- BERT / GPT.

## 1. Attention is All You Need

Instead of processing word-by-word (RNN), Transformers look at the **entire sentence at once**.
Self-Attention: Every word calculates its relationship with every other word.

## 2. Architecture

- **Encoder (BERT)**: Good for understanding (Classification, Q&A).
- **Decoder (GPT)**: Good for generation (Text Completion).

## Key Takeaways

- Transformers allow massive parallelization (unlike RNNs).
- This enabled the LLM revolution.
